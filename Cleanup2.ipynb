{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0144e82c",
   "metadata": {},
   "source": [
    "# Flight Data Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d9b3f",
   "metadata": {},
   "source": [
    "## Load source data into a single dataframe to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e749c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import zipfile\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18cf2514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Resources/BTS/BTS_Q1_2023.zip to Resources/BTS/extracted_zips\n",
      "Extracted Resources/BTS/BTS_Q2_2023.zip to Resources/BTS/extracted_zips\n",
      "Extracted Resources/BTS/BTS_Q3_2023.zip to Resources/BTS/extracted_zips\n",
      "Extracted Resources/BTS/BTS_Q4_2023.zip to Resources/BTS/extracted_zips\n"
     ]
    }
   ],
   "source": [
    "# List of all source zips\n",
    "zips = ['Resources/BTS/BTS_Q1_2023.zip','Resources/BTS/BTS_Q2_2023.zip',\n",
    "       'Resources/BTS/BTS_Q3_2023.zip','Resources/BTS/BTS_Q4_2023.zip']\n",
    "\n",
    "# Directory to extract to, confirm it exists\n",
    "extraction_path = 'Resources/BTS/extracted_zips'\n",
    "os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "# Iterate through zips and extract all\n",
    "for zip in zips:\n",
    "    with zipfile.ZipFile(zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extraction_path)\n",
    "\n",
    "    print(f\"Extracted {zip} to {extraction_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all extracted CSV's\n",
    "# glob is an efficient way to work through all CSV's in a directory: https://docs.python.org/3/library/glob.html\n",
    "csv_files = glob.glob(f\"{extraction_path}/*.csv\")\n",
    "\n",
    "# Empty list for storing DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through csv's and store into DataFrames list\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    dataframes.append(df)\n",
    "# Concatenante into single DataFrame\n",
    "unfiltered_flight_info = pd.concat(dataframes, ignore_index = True)\n",
    "unfiltered_flight_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns and remove limitation on display\n",
    "pd.set_option('display.max_columns', None)\n",
    "flights_df = pd.DataFrame(unfiltered_flight_info[['FlightDate','IATA_Code_Marketing_Airline', \n",
    "                                                  'Operated_or_Branded_Code_Share_Partners',\n",
    "                                                  'Operating_Airline ','Flight_Number_Marketing_Airline',\n",
    "                                                  'Origin','Dest','DepDelay','DepDelayMinutes',\n",
    "                                                  'DepDel15','ArrDelay','ArrDelayMinutes','Cancelled',\n",
    "                                                  'Diverted','CarrierDelay','WeatherDelay','NASDelay',\n",
    "                                                  'SecurityDelay','LateAircraftDelay']])\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c807325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty and delete the extraction_path directory\n",
    "try:\n",
    "    shutil.rmtree(extraction_path)\n",
    "    print(f\"Directory {extraction_path} and all its contents have been deleted.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error: {e.strerror}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84055094",
   "metadata": {},
   "source": [
    "## Explore and filter flights and airports to matching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e48bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique origin airports in data\n",
    "origins_df = unfiltered_flight_info[['Origin','OriginCityName']]\n",
    "origins_df['Origin'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique destination airports in data\n",
    "destinations_df = unfiltered_flight_info[['Dest','DestCityName']]\n",
    "destinations_df['Dest'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1812dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of unique airports in data\n",
    "# compare the origin airports to destination airports\n",
    "ori_match = origins_df['Origin'].isin(destinations_df['Dest'])\n",
    "dest_match = destinations_df['Dest'].isin(origins_df['Origin'])\n",
    "ori_dest_match = ori_match | dest_match\n",
    "\n",
    "# only keep rows for airports that are listed in both columns\n",
    "flight_airports = flights_df[ori_dest_match]\n",
    "flight_airports['Origin'].nunique()\n",
    "# confirmed that the same 359 airports have travel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09765e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV of cleaned us-airports and compare to airports in flight data\n",
    "us_airports = pd.read_csv('Resources/us-airports-cleaned.csv')\n",
    "iata_match = us_airports['iata_code'].isin(flight_airports['Origin'])\n",
    "\n",
    "us_airports_filtered = us_airports[iata_match]\n",
    "us_airports_filtered['iata_code'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc621a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which 8 airports do we have data for that our airport list doesn't have?\n",
    "iata_match = flight_airports['Origin'].isin(us_airports_filtered['iata_code'])\n",
    "\n",
    "us_airports_missing = flight_airports[~iata_match]\n",
    "us_airports_missing['Origin'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f7448",
   "metadata": {},
   "source": [
    "Research (Google):\n",
    "SJU, BQM, PSE = Puerto Rico\n",
    "STT,STX, = U.S. Virgin Islands\n",
    "PPG = American Samoa (unincorporated territory)\n",
    "GUM = Guam\n",
    "SPN = Northern Mariana Islands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding data from the 8 erroneous airports \n",
    "exclusions = flights_df['Origin'].isin(us_airports_missing['Origin'])\n",
    "\n",
    "filtered_flights = flights_df[~exclusions]\n",
    "filtered_flights['Origin'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter airports to just those that have flight data\n",
    "airport_mask = us_airports['iata_code'].isin(filtered_flights['Origin'])\n",
    "\n",
    "us_airports_filtered = us_airports[airport_mask]\n",
    "us_airports_filtered['iata_code'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118690c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export filtered data sets to CSV\n",
    "us_airports_filtered.to_csv('Resources/us_airports_filtered.csv')\n",
    "filtered_flights.to_csv('Resources/filtered_flights.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zip filtered_flights.csv and delete to allow github upload\n",
    "csv_path = 'Resources/filtered_flights.csv'\n",
    "zip_path = 'Resources/filtered_flights.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_path, os.path.basename(csv_path))\n",
    "\n",
    "# Check if the ZIP file exists and the size condition is met\n",
    "if os.path.exists(zip_path):\n",
    "    # Get the size of the ZIP file in bytes\n",
    "    zip_size = os.path.getsize(zip_path)\n",
    "    \n",
    "    # Define the size condition, e.g., file must be larger than 0 bytes to ensure it's not empty\n",
    "    if zip_size > 50:\n",
    "        os.remove(csv_path)\n",
    "        print(f\"The file {csv_path} has been zipped, is not empty, and has been deleted.\")\n",
    "    else:\n",
    "        print(f\"The file {zip_path} is empty.\")\n",
    "else:\n",
    "    print(\"Error: The zip file was not created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd828c8a",
   "metadata": {},
   "source": [
    "## Identify Airlines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9eb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44228a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Read in CSV and display a few rows\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# flight_csv = pd.read_csv('Resources/On_Time_Marketing_Carrier_On_Time_Performance_(Beginning_January_2018)_2023_1.csv', low_memory=False)\n",
    "# flight_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display columns without it being truncated because 'Operating_Airlines' column receives an error\n",
    "# print({col: index for index, col in enumerate(flight_csv.columns)})\n",
    "# # Discovered: 'Operating_Airline ' column has a space at the end in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20998dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select columns and create new dataframe \n",
    "# jan_flights_df = pd.DataFrame(flight_csv[['FlightDate','IATA_Code_Marketing_Airline', 'Operated_or_Branded_Code_Share_Partners',\n",
    "#                                       'Operating_Airline ','Flight_Number_Marketing_Airline','Tail_Number',\n",
    "#                                       'Origin','Dest','OriginCityName','DestCityName','CRSDepTime','DepTime',\n",
    "#                                       'DepDelay','DepDelayMinutes','DepDel15','ArrDelay','ArrDelayMinutes','Cancelled',\n",
    "#                                      'Diverted','CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay']])\n",
    "# jan_flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check Airlines included\n",
    "# print(jan_flights_df['IATA_Code_Marketing_Airline'].value_counts())\n",
    "# print(jan_flights_df['Operating_Airline '].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e951be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #exporting to csv\n",
    "# jan_flights_df.to_csv('Resources/jan_flights_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4039f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the ZIP file and the specific CSV file name inside the ZIP\n",
    "# source_zip = 'path/to/your/data.zip'\n",
    "# csv = 'your_file.csv'  # Name of the CSV file within the ZIP archive\n",
    "\n",
    "# # Use pandas.read_csv with the compression parameter\n",
    "# df = pd.read_csv(zip_file_path, compression='zip', encoding='utf-8', header=0, sep=',', quotechar='\"', error_bad_lines=False, engine='python', filepath_or_buffer=f\"zip://{zip_file_path}!{csv_file_name}\")\n",
    "\n",
    "# # Now you can work with the DataFrame `df` as usual\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c741e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Read in next CSV, select columns\n",
    "# column_selection = ['FlightDate','IATA_Code_Marketing_Airline', 'Operated_or_Branded_Code_Share_Partners',\n",
    "#                                       'Operating_Airline ','Flight_Number_Marketing_Airline','Tail_Number',\n",
    "#                                       'Origin','Dest','OriginCityName','DestCityName','CRSDepTime','DepTime',\n",
    "#                                       'DepDelay','DepDelayMinutes','DepDel15','ArrDelay','ArrDelayMinutes','Cancelled',\n",
    "#                                      'Diverted','CarrierDelay','WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay']\n",
    "# # more efficient way to work through multiple CSV's: https://docs.python.org/3/library/glob.html\n",
    "# csv_files = glob.glob('Resources/bts/*.csv')\n",
    "\n",
    "# # Empty list for DataFrames\n",
    "# dataframes = []\n",
    "\n",
    "# # Iterate through each CSV file and append to list\n",
    "# for file in csv_files:\n",
    "#     df = pd.read_csv(file, usecols=column_selection, low_memory = False)\n",
    "#     dataframes.append(df)\n",
    "\n",
    "# # Concatenate DataFrames from list to single df\n",
    "# combined_flight_info = pd.concat(dataframes, ignore_index=True)\n",
    "# combined_flight_info.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export to CSV\n",
    "# combined_flight_info.to_csv('Resources/combined_flight_info.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2121514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zip the CSV and delete to allow github upload\n",
    "# csv_path = 'Resources/combined_flight_info.csv'\n",
    "# zip_path = 'Resources/combined_flight_info.zip'\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#     zipf.write(csv_path, os.path.basename(csv_path))\n",
    "\n",
    "# # Confirm zip exists prior to deleting csv\n",
    "# if os.path.exists(zip_path):\n",
    "#     os.remove(csv_path)\n",
    "#     print(f\"The file {csv_file_path} has been zipped and deleted.\")\n",
    "# else:\n",
    "#     print(\"Error: The zip file was not created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de917093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import dependencies\n",
    "# import zipfile\n",
    "# import tempfile\n",
    "# import shutil\n",
    "# import glob\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read in source CSVs, select columns, concat info\n",
    "\n",
    "# # List of all source zips\n",
    "# zips = ['Resources/BTS/BTS_Q1_2023.zip','Resources/BTS/BTS_Q2_2023.zip',\n",
    "#        'Resources/BTS/BTS_Q3_2023.zip','Resources/BTS/BTS_Q4_2023.zip']\n",
    "# # Prepare to select columns\n",
    "# column_selection = ['FlightDate','IATA_Code_Marketing_Airline','Operating_Airline ',\n",
    "#                     'Flight_Number_Marketing_Airline','Tail_Number','Origin','Dest',\n",
    "#                     'CRSDepTime','DepTime','DepDelay','DepDelayMinutes','DepDel15','ArrDelay',\n",
    "#                     'ArrDelayMinutes','Cancelled', 'Diverted','CarrierDelay',\n",
    "#                     'WeatherDelay','NASDelay','SecurityDelay','LateAircraftDelay']\n",
    "# # Empty list for DataFrames\n",
    "# dataframes = []\n",
    "\n",
    "# # Loop through zips and extract to a temp directory\n",
    "# for zip in zips:\n",
    "#     with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#         with zipfile.ZipFile(zip, 'r') as zip_ref:\n",
    "#             zip_ref.extractall(temp_dir)\n",
    "        \n",
    "#         # Use glob to find all CSV files in the temporary directory\n",
    "#         # https://docs.python.org/3/library/glob.html\n",
    "#         csv_files = glob.glob(os.path.join(temp_dir, '*.csv'))\n",
    "        \n",
    "#         # Iterate through CSV files\n",
    "#         for csv_file in csv_files:\n",
    "#             df = pd.read_csv(csv_file, usecols=column_selection, low_memory = False)\n",
    "#             dataframes.append(df)\n",
    "        \n",
    "#         # combine the dataframes\n",
    "#         combined_flight_info = pd.concat(dataframes, ignore_index=True)\n",
    "#         # export to new csv\n",
    "#         combined_flight_info.to_csv('Resources/combined_flight_info.csv')\n",
    "# # Okay to close block and let temp_dir auto_delete\n",
    "# if os.path.exists('Resources/combined_flight_info.csv'):\n",
    "#     print(\"combined_flight_info.csv has been created\")\n",
    "# else: \n",
    "#     print(\"Something went wrong. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's test it without selecting columns at first. \n",
    "# # Read in source CSVs, select columns, concat info\n",
    "\n",
    "# # List of all source zips\n",
    "# zips = ['Resources/BTS/BTS_Q1_2023.zip','Resources/BTS/BTS_Q2_2023.zip',\n",
    "#        'Resources/BTS/BTS_Q3_2023.zip','Resources/BTS/BTS_Q4_2023.zip']\n",
    "# # Empty list for DataFrames\n",
    "# dataframes = []\n",
    "\n",
    "# # Loop through zips and extract to a temp directory\n",
    "# for zip in zips:\n",
    "#     with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#         with zipfile.ZipFile(zip, 'r') as zip_ref:\n",
    "#             zip_ref.extractall(temp_dir)\n",
    "        \n",
    "#         # Use glob to find all CSV files in the temporary directory\n",
    "#         # https://docs.python.org/3/library/glob.html\n",
    "#         csv_files = glob.glob(os.path.join(temp_dir, '*.csv'))\n",
    "        \n",
    "#         # Iterate through CSV files\n",
    "#         for csv_file in csv_files:\n",
    "#             df = pd.read_csv(csv_file, low_memory = False)\n",
    "#             dataframes.append(df)\n",
    "        \n",
    "#         # combine the dataframes\n",
    "#         combined_flight_info = pd.concat(dataframes, ignore_index=True)\n",
    "#         # export to new csv\n",
    "#         combined_flight_info.to_csv('Resources/combined_flight_info.csv')\n",
    "# # Okay to close block and let temp_dir auto_delete\n",
    "# if os.path.exists('Resources/combined_flight_info_unflitered.csv'):\n",
    "#     print(\"combined_flight_info_unfiltered.csv has been created\")\n",
    "# else: \n",
    "#     print(\"Something went wrong. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Zip the CSV and delete to allow github upload\n",
    "# csv_path = 'Resources/combined_flight_info.csv'\n",
    "# zip_path = 'Resources/combined_flight_info.zip'\n",
    "\n",
    "# with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "#     zipf.write(csv_path, os.path.basename(csv_path))\n",
    "\n",
    "# # Confirm zip exists prior to deleting csv\n",
    "# if os.path.exists(zip_path):\n",
    "#     os.remove(csv_path)\n",
    "#     print(f\"The file {csv_path} has been zipped and deleted.\")\n",
    "# else:\n",
    "#     print(\"Error: Zip failed to create.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
